{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAE Checkpoint Visualization\n",
        "\n",
        "This notebook loads a trained MAE checkpoint and visualizes reconstructions on sample galaxy images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/User/miniconda3/envs/DL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mae_model import create_mae_model\n",
        "from data import get_dataloaders\n",
        "from visualization import denormalize\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to your checkpoint (change this!)\n",
        "CHECKPOINT_PATH = \"models/Mae_Galaxy_Vit_Base_Epoch_400.pth\"  # Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Model configuration (must match your training config)\n",
        "model_config = {\n",
        "    \"image_size\": 256,\n",
        "    \"patch_size\": 16,\n",
        "    \"embed_dim\": 768,\n",
        "    \"encoder_depth\": 12,\n",
        "    \"encoder_heads\": 12,\n",
        "    \"decoder_embed_dim\": 512,\n",
        "    \"decoder_depth\": 8,\n",
        "    \"decoder_heads\": 16\n",
        "}\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Loading Galaxy10 dataset from Hugging Face...\n",
            "Calculating dataset statistics (mean and std)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Stats: 100%|██████████| 499/499 [00:54<00:00,  9.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calculated Mean: [0.16750076413154602, 0.16260723769664764, 0.15888828039169312]\n",
            "Calculated Std: [0.12320292741060257, 0.11179731786251068, 0.1046297699213028]\n",
            "\n",
            "Dataloaders created successfully.\n",
            "  - MAE loader: No augmentation (for reconstruction)\n",
            "  - Probe loaders: No augmentation (for evaluation)\n",
            "  - Fine-tune loaders: WITH augmentation (for training)\n",
            "Mean: [0.16750076413154602, 0.16260723769664764, 0.15888828039169312]\n",
            "Std: [0.12320292741060257, 0.11179731786251068, 0.1046297699213028]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load dataloaders\n",
        "print(\"Loading data...\")\n",
        "mae_loader, probe_train_loader, probe_test_loader, finetune_train_loader, finetune_test_loader, train_mean, train_std = get_dataloaders(\n",
        "    batch_size=32,\n",
        "    image_size=model_config[\"image_size\"],\n",
        "    num_workers=0  # Set to 0 for notebooks\n",
        ")\n",
        "\n",
        "print(f\"Mean: {train_mean}\")\n",
        "print(f\"Std: {train_std}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model & Checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model...\n",
            "Creating a randomly initialized MAE model...\n",
            "Loading checkpoint from models/Mae_Galaxy_Vit_Base_Epoch_400.pth...\n",
            "✓ Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "print(\"Creating model...\")\n",
        "mae_model = create_mae_model(**model_config)\n",
        "\n",
        "# Load checkpoint\n",
        "print(f\"Loading checkpoint from {CHECKPOINT_PATH}...\")\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
        "mae_model.load_state_dict(checkpoint)\n",
        "mae_model.to(DEVICE)\n",
        "mae_model.eval()\n",
        "\n",
        "print(\"✓ Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Reconstruction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_reconstruction(model, image, mean, std, device, title=\"MAE Reconstruction\"):\n",
        "    \"\"\"Visualize MAE reconstruction for a single image.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image_batch = image.unsqueeze(0).to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values=image_batch)\n",
        "        mask = outputs.mask.detach()\n",
        "        \n",
        "        # Get original patches\n",
        "        original_patches = model.patchify(image_batch)\n",
        "        \n",
        "        mask_expanded = mask.unsqueeze(-1)  # (batch, num_patches, 1)\n",
        "        masked_patches = original_patches * (1 - mask_expanded)  # Keep visible, zero masked\n",
        "        masked_image_tensor = model.unpatchify(masked_patches).squeeze(0)\n",
        "        \n",
        "        # --- Create Full Reconstruction (model's prediction for ALL patches) ---\n",
        "            # outputs.logits contains predictions for ALL patches\n",
        "        pred_patches = outputs.logits.detach()  # (B, N, P)\n",
        "\n",
        "        if getattr(model.config, \"norm_pix_loss\", True):\n",
        "            # compute per-patch stats from targets (i.e., original_patches)\n",
        "            patch_mean = original_patches.mean(dim=-1, keepdim=True)            # (B, N, 1)\n",
        "            patch_var  = original_patches.var(dim=-1, keepdim=True, unbiased=False)\n",
        "            patch_std  = (patch_var + 1e-6).sqrt()\n",
        "\n",
        "            # undo the per-patch normalization\n",
        "            pred_patches = pred_patches * patch_std + patch_mean  # back to dataset-normalized pixel space\n",
        "            \n",
        "        full_reconstruction_tensor = model.unpatchify(pred_patches).squeeze(0)\n",
        "\n",
        "        # --- Hybrid: visible from original + masked from reconstruction ---\n",
        "        hybrid_patches = original_patches * (1 - mask_expanded) + pred_patches * mask_expanded\n",
        "        hybrid_reconstruction_tensor = model.unpatchify(hybrid_patches).squeeze(0)\n",
        "\n",
        "        # --- Plotting ---\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        fig.suptitle(f'MAE Reconstruction at Epoch 400', fontsize=16)\n",
        "\n",
        "        original_vis = denormalize(image.cpu(), train_mean, train_std).permute(1, 2, 0).numpy()\n",
        "        masked_vis = denormalize(masked_image_tensor.cpu(), train_mean, train_std).permute(1, 2, 0).numpy()\n",
        "        full_recon_vis = denormalize(full_reconstruction_tensor.cpu(), train_mean, train_std).permute(1, 2, 0).numpy()\n",
        "        hybrid_vis = denormalize(hybrid_reconstruction_tensor.cpu(), train_mean, train_std).permute(1, 2, 0).numpy()\n",
        "\n",
        "        axs[0].imshow(original_vis); axs[0].set_title('Original'); axs[0].axis('off')\n",
        "        axs[1].imshow(masked_vis); axs[1].set_title('Masked (75%)'); axs[1].axis('off')\n",
        "        axs[2].imshow(full_recon_vis); axs[2].set_title('Full Reconstruction'); axs[2].axis('off')\n",
        "        axs[3].imshow(hybrid_vis); axs[3].set_title('Hybrid (Visible + Recon)'); axs[3].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get a Sample Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: torch.Size([3, 256, 256])\n",
            "Galaxy class: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/User/miniconda3/envs/DL/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# Get one image from the test set\n",
        "test_batch = next(iter(probe_test_loader))\n",
        "test_image = test_batch['pixel_values'][0]  # Get first image\n",
        "test_label = test_batch['label'][0].item()\n",
        "\n",
        "print(f\"Image shape: {test_image.shape}\")\n",
        "print(f\"Galaxy class: {test_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the reconstruction\n",
        "visualize_reconstruction(\n",
        "    mae_model,\n",
        "    test_image,\n",
        "    train_mean,\n",
        "    train_std,\n",
        "    DEVICE,\n",
        "    title=f\"Galaxy Class {test_label}\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
